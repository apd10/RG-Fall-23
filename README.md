# Reading group for Fall 23 
This reading group will go on from mid August till first week of december. We have approximately 14 weeks which means we can hold 28 presentations in total.
# Relevant Links
1. ([webpage link](https://apd10.github.io/RG-Fall-23/))
2. Sign up sheet ([spreadsheet](https://docs.google.com/spreadsheets/d/1d2h9ndz9gtsQKnr7BkshowLjWjBSavc2pTqVGHs97uM/edit#gid=0))
3. Unanswered questions doc ([doc](https://docs.google.com/document/d/1hfLuDhQOH9LCGR1zcUuVny1o1aEK2BpQevwe6Ma60mc/edit?usp=sharing))

Contributors

|        **Name**        | **Role** |
|:-----------------------:|:-----------:|
| Aditya Desai | TBD |
| Zhenghui Guo | R2  |
| Gaurav Gupta | R3  |
| Masa Maksimovic | R2 |
| Atishay Jain | R1  |
| Sanya Garg | R2 |
| Benjamin Meisburger | R2 |

|        **Name**        | **Title** | **Suggested to be read by** | **date** | **Unanswered questions updated?** | slides |
|:-----------------------:|:-----------:|:-----------:|:--------:|:--------:|:--------:|
| Aditya Desai | Moments, deviations, chernoff/Hoeffding | All | 08/28/2023 | Yes | N.A |
| Tony Zhang | Near neighbor graph, inner product transformation, inner product search | All | 09/06/2023 | &#10071; | &#10071; |
| Jonah Yi | Learning to Route | All | 09/06/2023 | &#10071; | &#10071; |
| Ben Meisburger | Deep Gradient Compression | All | 09/11/2023 | Yes | [Slides](https://docs.google.com/presentation/d/13_4C_rVIMUgSwd4_h7OzJvE_ATiirp2cv46h4-4ugaQ/edit?usp=sharing) |
| Masa Maksimovic | Convergence in ML | All | 09/11/2023 | Yes | Attached above as .pdf |
| Zhenghui Guo(Kevin) | KV cache inference | All | 09/18/2023 | &#10071; | &#10071; |

# Announcements (Format -  Title : Date of announcement) . Please put latest first. 

## Presentation by Zhenghui Guo(kevin): 18th Sept.
1. Efficient Memory Management for Large Language Model Serving with PagedAttention: https://arxiv.org/pdf/2309.06180.pdf
 
## Presentation by Ben Meisburger: 11th Sept.
1. Deep Gradient Compression: https://openreview.net/pdf?id=SkhQHMW0W

## Presentation by Masa Maksimovic: 11th Sept.
1. Convergence proofs for some simple settings: https://akyrillidis.github.io/comp414-514/schedule/?fbclid=IwAR1_ImKSLRQFgEGENLVBJD5stwFKf7fogHwse-w-NBrHxFVdMHl7iLRMUVA
2. A convergence theory for deep learning: https://arxiv.org/abs/1811.03962v1

## Presentation by Tony Zhang : 1st Sept.
1. Hierarchical Navigable Small World (HNSW) data structure for approximate near neighbor search: https://arxiv.org/abs/1603.09320
2. Vector transformation for making them more amenable to inner product search: https://arxiv.org/abs/1405.5869
3. IP-NSW for maximum inner product search: https://proceedings.neurips.cc/paper/2018/hash/229754d7799160502a143a72f6789927-Abstract.html

## Presentation by Jonah Yi : 1st Sept.
Learning to Route (LTR) in Similarity Graphs: https://arxiv.org/abs/1905.10987

## Location Confirmed! DH1049 : 30th August
We have confirmed booking for DH1049 every monday (except Oct 16 where we will book a library room)

## Presentation by Aditya Desai : 26th August
I will aim to cover parts of chapter 3 and chapter 4 from http://lib.ysu.am/open_books/413311.pdf

## Roles in Fall 23 : 9th August 2023

We have the following roles in this edition.

### R1:
This is focused research presentations. You will have to declare a particular problem that you are looking at and make three presentations: 
1. The first presentation will be motivating the problem (signifiance, impact, urgency, etc) and some related work that people have done. (Example :  Embedding table compression is important, why, how have people solved this in the past, what problems remain, etc)
2. Fundamentals of the approach. This is a "teaching presentation" where you explain the fundamental toolkit that you are using. (Example: I am going to use random projection based approach. what are random projections, how to analyse their usefulness, what are hash functions, why they are important, analysing hash functions)
3. Your proposed method and results . (Example, the recipe to apply random projection based compression in embedding tables, results - quality, efficiency, etc)


### R2:
This is more general research presentations. You will pick an area of research (for example, training efficiency, model compression, ml on edge, etc). You ll be making 2 presentations. This is similar to how things ran in summer edition of reading group. 
You ll  keep reading papers on a single topic and summarize your learnings in two presentations. 

### R3:
This is a role of "moderating presentations". You ll not be presenting yourself. But you ll shadow readings of others if possible and actively discuss while they are presenting. The goal of this role is to keep the presentations engaging and to get the best out of it.

### R4: 
This is a role of "general audience and shadow". You ll read papers from one of the broad topics shadowing other presenters, listen in to presentations and participate in discussions.

&#10071; Add your name to the table above about roles.

## How to participate : 9th August 2023 &#10071;&#10071;
1. If you are participating in the reading group, send me your github id at apd10@rice.edu . I will add you to the collaborators so that you can change the readme and thus update the webpage.
2. It will a collaborative effort to keep this webpage updated. So make sure to get yourself added to collaborators and update portions that are relevant to you over the period of entire edition.
   
